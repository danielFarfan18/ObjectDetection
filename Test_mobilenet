import pyrealsense2 as rs
import numpy as np
import cv2
from multiprocessing import Process
from datetime import datetime
import RPi.GPIO as GPIO

def px2grados(px):
    # Constantes de calibración
    px_cal = 640
    dx_cal = 1.015
    dz_cal = 0.958
    pz_cal = (px_cal / dx_cal) * dz_cal
    px_centro = px_cal / 2

    # Cálculo del ángulo
    delta = px - px_centro
    rad = np.arctan(delta / pz_cal)
    grados = rad * 180 / np.pi
    return grados

# Ruta de trabajo y configuración inicial
wd = '/home/nachox99/Documents/240_Detector_Redes/240_Detector_Redes/'

# Carga de las categorías de la red
classNames = []
classFile = f'{wd}ssd_mobilenet_v3_large_coco_2020_01_14/categorias.txt'
with open(classFile, 'rt') as f:
    classNames = f.read().rstrip('\n').split('\n')

# Se carga y configura la red
configPath = f'{wd}ssd_mobilenet_v3_large_coco_2020_01_14/ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt'
weightsPath = f'{wd}ssd_mobilenet_v3_large_coco_2020_01_14/frozen_inference_graph.pb'

# Variable global para controlar la ejecución de los hilos
stop_threads = False

# Inicialización de GPIO
GPIO.setmode(GPIO.BOARD)  # Usa la numeración física de los pines
led_pins = [11, 12, 13]  # Lista de pines GPIO para los LEDs

# Configuración de los pines como salida y apagado inicial
for pin in led_pins:
    GPIO.setup(pin, GPIO.OUT)
    GPIO.output(pin, GPIO.LOW)

def start_camera_pipeline(serial_number, led_pin):
    # Instanciar una red propia para cada hilo
    net = cv2.dnn_DetectionModel(weightsPath, configPath)
    net.setInputSize(320, 320)
    net.setInputScale(1.0 / 127.0)
    net.setInputMean((127.5, 127.5, 127.5))
    net.setInputSwapRB(True)

    if cv2.cuda.getCudaEnabledDeviceCount() > 0:
        print("Usando CUDA...")
        net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)
        net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)
    else:
        print("CUDA no está disponible; usando CPU.")

    # Configuración de la cámara RealSense específica
    pipeline = rs.pipeline()
    config = rs.config()
    config.enable_device(serial_number)
    config.enable_stream(rs.stream.depth, 640, 360, rs.format.z16, 30)
    config.enable_stream(rs.stream.color, 640, 360, rs.format.bgr8, 30)
    pipeline.start(config)

    
    
    # Configuración de alineación
    align_to = rs.stream.color
    align = rs.align(align_to)

    fps_viejo = 20
    t_inicio = datetime.now()

    # Crear filtros de profundidad
    decimation = rs.decimation_filter()
    depth_to_disparity = rs.disparity_transform(True)
    spatial = rs.spatial_filter()
    temporal = rs.temporal_filter()
    disparity_to_depth = rs.disparity_transform(False)

    try:
        t_inicio = datetime.now() 
        fps_viejo = 20 
        global stop_threads
        while not stop_threads:
            frames = pipeline.wait_for_frames()
            aligned_frames = align.process(frames)
            depth_frame = frames.get_depth_frame()
            color_frame = frames.get_color_frame()
            if not color_frame:
                continue
            
            color_image = np.asanyarray(color_frame.get_data())

            # Aplicar los filtros
            frame = decimation.process(depth_frame)
            frame = depth_to_disparity.process(frame)
            frame = spatial.process(frame)
            frame = temporal.process(frame)
            frame = disparity_to_depth.process(frame)
            
            # Convertir la profundidad a colormap para visualización
            colorizer = rs.colorizer()
            depth_colormap = np.asanyarray(colorizer.colorize(frame).get_data())

            color_image = np.asanyarray(color_frame.get_data())
            outputs = net.detect(color_image, confThreshold=0.5)
            classIds, confs, bbox = outputs

            # Si los datos no son ndarray, conviértelos
            if not isinstance(classIds, np.ndarray):
                classIds = np.array(classIds)
            if not isinstance(confs, np.ndarray):
                confs = np.array(confs)
            if not isinstance(bbox, np.ndarray):
                bbox = np.array(bbox)

            # Filtrar solo personas (1) y autos (3)
            object_detected = False
            for classId, conf, box in zip(classIds.flatten(), confs.flatten(), bbox):
                if classId == 1 or classId == 3:  # 1 es 'persona', 3 es 'auto'
                    x, y, w_, h_ = box
                    cx = x + (w_ // 2)
                    cy = y + (h_ // 2)

                    # Cálculo de distancia y ángulo
                    distance = depth_frame.get_distance(cx, cy)
                    if distance < 1.0:  # Si la distancia es menor a 1 metro
                        object_detected = True
                        #print(f"Objeto detectado a menos de 1 metro. Distancia: {distance:.2f}m")

                    alpha = px2grados(cx)
                    cv2.rectangle(color_image, box, color=(0, 255, 0), thickness=2)
                    text = f'{classNames[classId-1]}: {conf:.2f}'
                    cv2.putText(color_image, text, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
                    cv2.circle(color_image, (cx, cy), 5, (0, 255, 0), -1)
                    text = f'{classNames[classId - 1]}: {conf:.2f}, Dist: {distance:.2f}m, Angle: {alpha:.2f}°'
                    cv2.putText(color_image, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

            # Control del LED basado en la detección
            if object_detected:
                GPIO.output(led_pin, GPIO.HIGH)
                #print(f"Encendiendo LED en el pin {led_pin}")
            else:
                GPIO.output(led_pin, GPIO.LOW)
                #print(f"Apagando LED en el pin {led_pin}")
                    
            cv2.imshow(f'RealSense Color {serial_number}', color_image)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                stop_threads = True
                break
    finally:
        pipeline.stop()
        GPIO.output(led_pin, GPIO.LOW)  # Apagar el LED cuando se detiene el pipeline
        cv2.destroyAllWindows()

def get_realsense_serial_numbers():
    ctx = rs.context()
    devices = ctx.query_devices()
    serial_numbers = [dev.get_info(rs.camera_info.serial_number) for dev in devices]
    return serial_numbers

if __name__ == "__main__":
    serial_numbers = get_realsense_serial_numbers()
    processes = [Process(target=start_camera_pipeline, args=(sn, pin)) for sn, pin in zip(serial_numbers, led_pins)]

    for p in processes:
        p.start()

    for p in processes:
        p.join()
        
    # Cleanup GPIO
    GPIO.cleanup()

